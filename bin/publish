#!/usr/bin/env python3
import argparse
import hashlib
import json
import typing as T
from getpass import getpass
from pathlib import Path
from subprocess import run

import requests

ROOT_DIR = Path(__file__).parent.parent
IGNORE = [".git"]

USER_NAME = "old-castle-fansubs"
ALLOWED_FILETYPES = [
    ".html",
    ".htm",
    ".jpg",
    ".png",
    ".webm",
    ".gif",
    ".svg",
    ".ico",
    ".js",
    ".css",
    ".txt",
    ".ass",
]


def collect_local_files(local_root: Path) -> T.Iterable[Path]:
    for path in local_root.iterdir():
        if path.name in IGNORE:
            continue
        elif path.is_file():
            if path.suffix in ALLOWED_FILETYPES:
                yield path
        elif path.is_dir():
            yield from collect_local_files(path)


def get_local_info(local_paths: T.Iterable[Path]) -> T.Dict[Path, str]:
    return {
        path: hashlib.md5(path.read_bytes()).hexdigest()
        for path in local_paths
    }


def get_remote_info(published_path: Path) -> T.Dict[Path, str]:
    if not published_path.exists():
        return {}
    return {
        Path(path): checksum
        for path, checksum in json.loads(published_path.read_text()).items()
    }


def save_remote_info(published_path: Path, info: T.Dict[Path, str]) -> None:
    published_path.write_text(
        json.dumps({str(path): checksum for path, checksum in info.items()})
    )


def collect_diff(
    local_info: T.Dict[Path, str], remote_info: T.Dict[Path, str]
) -> T.Tuple[T.Set[Path], T.Set[Path], T.Set[Path]]:
    local_paths = set(local_info.keys())
    remote_paths = set(remote_info.keys())

    files_to_create = local_paths - remote_paths
    files_to_delete = remote_paths - local_paths
    files_to_update = set(
        path
        for path in local_paths & remote_paths
        if local_info[path] != remote_info[path]
    )

    return files_to_create, files_to_update, files_to_delete


def sync_files(
    base_url: T.Optional[str],
    local_root: Path,
    files_to_upload: T.Set[Path],
    files_to_delete: T.Set[Path],
) -> None:
    if files_to_delete:
        for path in sorted(files_to_delete):
            print(f"Removing {path}")
            if base_url:
                response = requests.post(
                    f"{base_url}/delete",
                    params={"filenames[]": str(path.relative_to(local_root))},
                )
                response.raise_for_status()

    if files_to_upload:
        for path in sorted(files_to_upload):
            print(f"Uploading {path}")
            files = {str(path.relative_to(local_root)): path.open(mode="rb")}
            if base_url:
                response = requests.post(f"{base_url}/upload", files=files)
                response.raise_for_status()


def wipe_cache() -> None:
    print("Wiping reverse proxy cache")
    run(["ssh", "oc", "sudo", "rm", "-rf", "/var/nginx/cache/"])
    run(["ssh", "oc", "sudo", "systemctl", "restart", "nginx"])


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dry-run", action="store_true")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    dry_run = args.dry_run

    local_root = ROOT_DIR / "public"
    published_path = ROOT_DIR / ".publish.json"

    local_files = list(collect_local_files(local_root))
    local_info = get_local_info(local_files)
    remote_info = get_remote_info(published_path)

    files_to_create, files_to_update, files_to_delete = collect_diff(
        local_info, remote_info
    )
    files_to_upload = files_to_update | files_to_create

    if not files_to_upload and not files_to_delete:
        print("Nothing to do")
        return

    if dry_run:
        base_url = None
    else:
        password = getpass("Password: ")
        base_url = f"https://{USER_NAME}:{password}@neocities.org/api"

    sync_files(base_url, local_root, files_to_upload, files_to_delete)

    if not dry_run:
        save_remote_info(published_path, local_info)
        wipe_cache()


if __name__ == "__main__":
    main()
